{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Two-Layer Dense Neural Network Classifier on Spiral Data\n",
    "\n",
    "We will be two-layer dense neural network, using a cross-entropy loss, on our toy dataset of points. After training our model, we will be able to visualize the decision boundary that it learned!\n",
    "\n",
    "Two-layer Model of dense layers:\n",
    "- Layer 1: dense mapping with a ReLU activation.\n",
    "  - Layer 1 has 100 neurons: $W_{1}$ has the shape (2, 100), $b_{1}$ has the shape (100,) \n",
    "  - $f_{1}(X; W_{1},b_{1}) = ReLU(XW_{1} + b_{1})$\n",
    "- Layer 2: dense mapping with a softmax activation\n",
    "  -  Layer 2 has 3 neurons: $W_{2}$ has the shape (100, 3), $b_{2}$ has the shape (3,) \n",
    "  - $f_{2}(X; W_{2}, b_{2}) = softmax(XW_{2} + b_{2})$\n",
    "\n",
    "#### Composing Layers\n",
    "$f_{model}(X; W_{1}, W_{2}, b_{1}, b_{2}) = f_{2}(f_{1}(X; W_{1}, b_{1}); W_{2}, b_{2})$\n",
    "\n",
    "#### The Model in Full\n",
    "$f(X; W_{1}, W_{2}, b_{1}, b_{2}) = softmax(ReLU(XW_{1} + b_{1})W_{2} + b_{2})$\n",
    "\n",
    "Loss:\n",
    "- cross-entropy loss\n",
    "  - $L_{i} = -\\sum_{k=0}^{2}{p^{(i)}_{k} \\log{q^{(i)}_{k}}}$\n",
    " \n",
    " > $p^{(i)}$ is the **true** probability-distribution for classification. E.g. $p^{(i)}= (0, 0, 1)$\n",
    "    if datum $i$ belongs to tendril 2.\n",
    "    \n",
    "  > $q^{(i)}$ is the **predicted** probability-distribution for classification. E.g. $q^{(i)} = (0.1, 0.1, 0.8)$ predicts that datum $i$ is class 2 with a 80% probability.\n",
    "    if datum $i$ belongs to tendril 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from jupyterthemes import jtplot\n",
    "    jtplot.style()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import ToyData\n",
    "toy_data = ToyData(num_classes=3)\n",
    "xtrain, ytrain, xtest, ytest = toy_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_data.plot_spiraldata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Our Model Parameters\n",
    "We will be using a intialization technique known as \"He-normal\" initialization (pronounced \"hey\"). Basically we draw all of our W-values from a normal distribution, but which has been scaled by $\\frac{1}{\\sqrt{2N_{row}}}$, where $N_{row}$ is the number of rows in $W$.\n",
    "\n",
    "We need to take care in our initialization since we are now working with many more W-parameters (2, 100) and (100, 3), instead of (2, 3)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def he_normal(shape):\n",
    "    \"\"\" Given the desired shape of your array, draws random\n",
    "        values from a scaled-Gaussian distribution.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\"\"\"\n",
    "    N = shape[0]\n",
    "    scale = 1 / np.sqrt(2*N)\n",
    "    return np.random.randn(*shape)*scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(param, rate):\n",
    "    \"\"\" Performs a gradient-descent update on the parameter.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        param : mygrad.Tensor\n",
    "            The parameter to be updated.\n",
    "        \n",
    "        rate : float\n",
    "            The step size used in the update\"\"\"\n",
    "    param.data -= rate*param.grad\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model_out, labels):\n",
    "    \"\"\" Computes the mean accuracy, given predictions and true-labels.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model_out : numpy.ndarray, shape=(N, K)\n",
    "            The predicted class-scores/probabilities\n",
    "        labels : numpy.ndarray, shape=(N, K)\n",
    "            The one-hot encoded labels for the data.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The mean classification accuracy of the N samples.\"\"\"\n",
    "    return np.mean(np.argmax(model_out, axis=1) == np.argmax(labels, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy Loss\n",
    "Because we are taking the soft-max at the end of our neural network, the (N,3) scores that we compute can actually be interpreted as (N, 3) probabilities. Just as with the scores, the largest probability determines what class we predict. For instance, we might get the following for our **predicted** probability distribution.\n",
    "\n",
    "```\n",
    "p_pred = [[0.1,  0.4,  0.5],\n",
    "          [0.9, 0.05, 0.05],\n",
    "          ...]\n",
    "```\n",
    "\n",
    "Which means the datum-0 is predicted to be in tendril-2 (with 50% probability), datum-1 is predicted to be in tendril-0 (with 90% probability). Etc.\n",
    "\n",
    "Our one-hot labels give us the \"true\" probability distribution. The correct class is 100% correct, the wrong classes have 0% probability of being correct:\n",
    "\n",
    "```\n",
    "p_true = [[0, 0, 1],\n",
    "          [1, 0, 0],\n",
    "          ...]\n",
    "```\n",
    "\n",
    "Cross-entropy measures how **dissimilar** two probability distributions are. We want our predicted probabilities to be close to our true-distribution. Thus a large cross entropy, meaning they are dissimilar, is bad. A small cross-entropy, meaning the distributions are similar, is good. This is also differentiable function - all the makings of a great loss function!\n",
    "\n",
    "The following is the cross-entropy loss for datum-i. $p$ is the **true** probability distribution. $q$ is the **predicted** probability distribution. The sum is over the different possible classes:\n",
    "\n",
    "\\begin{equation}\n",
    "L_{i} = -\\sum_{k=0}^{2}{p^{(i)}_{k} \\log{q^{(i)}_{k}}}\n",
    "\\end{equation}\n",
    "\n",
    "Thus our total cross-entropy loss is just the mean-value of these for our N-values.\n",
    "\\begin{equation}\n",
    "L = \\frac{1}{N}\\sum_{i=0}^{N-1}{L_{i}} = -\\frac{1}{N}\\sum_{i=0}^{N-1}\\sum_{k=0}^{2}{p^{(i)}_{k} \\log{q^{(i)}_{k}}}\n",
    "\\end{equation}\n",
    "\n",
    "Let's code this up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mygrad.math import log\n",
    "def cross_entropy(p_pred, p_true):\n",
    "    \"\"\" Computes the mean cross-entropy.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        p_pred : mygrad.Tensor, shape:(N, K)\n",
    "            N predicted distributions, each over K classes.\n",
    "        \n",
    "        p_true : mygrad.Tensor, shape:(N, K)\n",
    "            N 'true' distributions, each over K classes\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor, shape=()\n",
    "            The mean cross entropy (scalar).\"\"\"\n",
    "    \n",
    "    N = p_pred.shape[0]\n",
    "    p_logq = (p_true) * log(p_pred)\n",
    "    return (-1/ N) * p_logq.sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mygrad.nnet.layers import dense\n",
    "from mygrad.nnet.activations import softmax, relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a learning rate of `1.` and **no** regularization, train your model for 1000 iterations. Record the loss and accuracy for each operation. Plot them afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mygrad import Tensor\n",
    "\n",
    "D = 2  # dimensionality of a piece of input data\n",
    "K = 3  # number of distinct classes\n",
    "\n",
    "W1 = Tensor(he_normal((D, 100)))\n",
    "b1 = Tensor(np.zeros((100,), dtype=W1.dtype))\n",
    "W2 = Tensor(he_normal((100, K)))\n",
    "b2 = Tensor(np.zeros((K,), dtype=W2.dtype))\n",
    "params = [b1, W1, b2, W2]\n",
    "\n",
    "rate = 1.\n",
    "\n",
    "l = []\n",
    "acc = []\n",
    "for i in range(1000):\n",
    "    o1 = relu(dense(xtrain, W1) + b1)\n",
    "    p_pred = softmax(dense(o1, W2) + b2)\n",
    "    \n",
    "    # Li = -1 * sum_over_classes(p_true * log(p_predict))\n",
    "    # L = 1/N sum_over_i(Li)\n",
    "    loss = cross_entropy(p_pred=p_pred, p_true=ytrain)\n",
    "    \n",
    "    l.append(loss.data.item())\n",
    "    loss.backward()\n",
    "\n",
    "    acc.append(compute_accuracy(p_pred.data, ytrain))\n",
    "    \n",
    "    for param in params:\n",
    "        sgd(param, rate)\n",
    "    \n",
    "    loss.null_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot training  performance\n",
    "fig,(ax,ax2) = plt.subplots(nrows=2)\n",
    "ax.plot(l)\n",
    "ax2.plot(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fwd_pass(x):\n",
    "    \"\"\" Computes the forward-pass of our model, using numpy arrays\n",
    "        since we don't need to bother with back-prop when computing\n",
    "        predictions.\"\"\"\n",
    "    o1 = relu(dense(x, W1.data) + b1.data)\n",
    "    o2 = softmax(dense(o1, W2.data) + b2.data)\n",
    "    return o2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_data.visualize_model(fwd_pass, entropy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_data.visualize_model(fwd_pass, entropy=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
